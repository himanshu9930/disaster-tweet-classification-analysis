# ğŸŒªï¸ Disaster Tweet Classification using NLP & BERT

This project tackles the real-world challenge of distinguishing disaster-related tweets from irrelevant content during emergencies. It was developed as part of the **MGMT 590 â€“ Machine Learning** course at Purdue University.

## ğŸ“ Project Goal

To build an effective Natural Language Processing (NLP) pipeline capable of classifying tweets as disaster-related or not, using state-of-the-art deep learning techniques. The best-performing model achieved an **F1 score of 0.844** and ranked **22nd on the Kaggle leaderboard**.

## ğŸ“Š Dataset

- Source: [Kaggle - Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started)
- Contains 10,000+ tweets labeled as `1` (disaster) or `0` (non-disaster)
- No significant class imbalance detected

## ğŸ” Key Techniques

### ğŸ§ª Exploratory Data Analysis
- Disaster tweets contain slightly more words on average
- Clear difference in vocabulary between classes
- No need for resampling due to balanced target variable

### ğŸ› ï¸ Preprocessing
- Standard NLP steps: lowercasing, tokenization, punctuation & stopword removal
- Multiple embedding strategies evaluated: GloVe + LSTM, BERT, GCN-based embeddings

### ğŸ¤– Models Used
- **GloVe + LSTM** â€“ F1 Score: 0.795
- **GCN Embedding + Classifier** â€“ F1 Score: 0.797
- **BERT (bert-large-uncased)** â€“ F1 Score: **0.844**

> ğŸ“Œ **Winner**: BERT with bi-directional embeddings and subword tokenization delivered the best performance.

## ğŸ§  Why BERT?
- Pre-trained contextual understanding
- Bi-directional, non-static embeddings
- Robust performance with fewer epochs
- Best suited for sentence-level classification in short texts like tweets

## âš™ï¸ Technologies Used

- Python (Transformers, PyTorch, Scikit-learn)
- HuggingFace `transformers` library for BERT
- Pandas, Seaborn, Matplotlib for EDA
- Jupyter Notebooks

## ğŸš§ Challenges Faced

- BERTâ€™s large model size required significant compute power
- Hyperparameter tuning risked overfitting
- Graph-based embeddings were experimental and harder to tune

## ğŸ“ˆ Final Results

| Model                | F1 Score | Kaggle Rank |
|---------------------|----------|-------------|
| GloVe + LSTM        | 0.795    | â€“           |
| GCN Embeddings      | 0.797    | â€“           |
| **BERT (final model)** | **0.844** | **22**        |

## ğŸ‘¥ Contributors

- **Tushar Malankar**  
- **Himanshu Sharma**

## ğŸ“¬ Contact

Feel free to connect for collaboration or discussion:

- âœ‰ï¸ [tmalanka@purdue.edu](mailto:tmalanka@purdue.edu)
- ğŸ”— [LinkedIn â€“ Tushar Malankar](https://www.linkedin.com/in/tushar-malankar)

